{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying MNIST digits with a softmax classifier\n",
    "# Name:Şeyhmus Aydoğdu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a softmax classifier to predict the digit presented in a given image. We will use the MNIST dataset for this task. Please first skim through the notebook. Then complete the following steps mentioned in the main function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. minibatch\n",
    "2. init_params\n",
    "3. forward and backward propagation\n",
    "    * softmax_forw\n",
    "    * softmax_back_and_loss\n",
    "4. grad_check\n",
    "5. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; for p in [\"Knet\"]; haskey(Pkg.installed(),p) || Pkg.add(p); end #Knet installation to use the MNIST dataset\n",
    "using Knet, Printf, Random\n",
    "import Knet: minibatch, accuracy\n",
    "include(Knet.dir(\"data\", \"mnist.jl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minibatch (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takes raw input (X) and gold labels (Y)\n",
    "# returns list of minibatches (x, y)\n",
    "function minibatch(X, Y, bs=100)\n",
    "    data = Any[]\n",
    "    for i=1:bs:size(X,2)\n",
    "         index = i + bs -1 \n",
    "         j= min(index, size(X,2))\n",
    "        push!(data, ((X[:,i:j]), Y[:,i:j]))\n",
    "    end\n",
    "    return data\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_params (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init_params(ninputs, noutputs)\n",
    "# this functions takes two arguments,\n",
    "#   number of input features (ninputs::Int)\n",
    "#   and number of output classes (noutputs::Int)\n",
    "# returns randomly generated W (sampled from N(0,1e-3))\n",
    "#   and b(must be zeros vector) params of softmax\n",
    "function init_params(ninputs, noutputs)\n",
    "    b = zeros(noutputs,1)\n",
    "    W = randn(noutputs,ninputs) .* 1e-3\n",
    "    return W,b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_forw (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax_forw(W, b, x)\n",
    "# this function takes three arguments,\n",
    "#   model weights (W,b) and single minibatch input data (x)\n",
    "# applies the affine transformation and softmax function\n",
    "# returns predicted probabilities\n",
    "function softmax_forw(W, b, x)\n",
    "    return exp.((W * x) .+ b) ./ sum(exp.((W * x) .+ b), dims=1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_back_and_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax_back_and_loss(W, b, x, ygold)\n",
    "# takes model weights (W,b), input data (x) and correct labels (ygold)\n",
    "# calculates and returns the soft loss, gradients of w and b\n",
    "function softmax_back_and_loss(W, b, x, ygold)   \n",
    "    loss = -sum(ygold .* (log.(softmax_forw(W,b,x)))) / size(x,2)\n",
    "    grad_b = sum( -(ygold-softmax_forw(W,b,x)) / size(x,2), dims=2)\n",
    "    grad_W =  -(ygold-softmax_forw(W,b,x)) / size(x,2) * x'\n",
    "    return loss, grad_W, grad_b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad_check (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grad_check(W, b, x, ygold)\n",
    "# takes model weights (W,b), one minibatch input data (x) and\n",
    "#   and true labels (ygold) as input parameters\n",
    "# displays info about whether your gradient calculation procedure\n",
    "#   passes the gradient check test or not.\n",
    "function grad_check(W, b, x, ygold)\n",
    "    # numeric_gradient()\n",
    "    # calculates and returns numeric gradients of model weights (gw,gb)\n",
    "    function numeric_gradient()\n",
    "        epsilon = 0.0001\n",
    "\n",
    "        gw = zeros(size(W)) # gradient of W\n",
    "        gb = zeros(size(b)) # gradient of b\n",
    "\n",
    "        # Your code here\n",
    "        gw = ((W .* (gw .+ epsilon)) - (W  .* (gw .- epsilon))) / (2 * epsilon)\n",
    "        gb = ((b .* (gb .+ epsilon)) - (b  .* (gb .- epsilon))) / (2 * epsilon)  \n",
    "\n",
    "        return gw, gb\n",
    "    end\n",
    "\n",
    "    _,gradW,gradB = softmax_back_and_loss(W, b, x, ygold)\n",
    "    gw, gb = numeric_gradient()\n",
    "\n",
    "    diff = sqrt(sum((gradW - gw) .^ 2) + sum((gradB - gb) .^ 2))\n",
    "    println(\"Diff: $diff\")\n",
    "    if diff < 1e-7\n",
    "        println(\"Gradient Checking Passed\")\n",
    "    else\n",
    "        println(\"Diff must be < 1e-7\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 2 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train(W, b, data, lr=0.15)\n",
    "# takes model weights (W,b), data split (data) and learning rate (lr) as input\n",
    "# iterates over the whole data split and in each iteration network is updated\n",
    "# by using the gradients obtained from softmax_back_and_loss function call. default learning\n",
    "# rate is 0.15. it returns the average softloss over the whole data split.\n",
    "function train(W, b, data, lr=0.15)\n",
    "    totalcost = 0.0\n",
    "    numins = 0\n",
    "    for (x, y) in data\n",
    "        cost, grad_W, grad_b =softmax_back_and_loss(W, b, x, y)        \n",
    "        W  .= W - lr * grad_W\n",
    "        b  .= b - lr * grad_b\n",
    "        totalcost += cost .* size(x,2)\n",
    "        numins += size(x,2)\n",
    "    end\n",
    "    avgcost = totalcost / numins\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 4 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't touch this cell. Read it carefully.\n",
    "# accuracy(ygold, ypred)\n",
    "# takes true labels (ygold) and predicted scores as input for single minibatch.\n",
    "# returns accuracy\n",
    "function accuracy(ygold, ypred)\n",
    "    correct = 0.0\n",
    "    for i=1:size(ygold, 2)\n",
    "        correct += findmax(ygold[:,i]; dims=1)[2] == findmax(ypred[:, i]; dims=1)[2] ? 1.0 : 0.0\n",
    "    end\n",
    "    return correct / size(ygold, 2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't touch this cell. Read it carefully.\n",
    "function main()\n",
    "    Random.seed!(12345)\n",
    "\n",
    "    # Size of input vector (MNIST images are 28x28)\n",
    "    ninputs = 28 * 28\n",
    "\n",
    "    # Number of classes (MNIST images fall into 10 classes)\n",
    "    noutputs = 10\n",
    "\n",
    "    ### Data loading & preprocessing\n",
    "    #\n",
    "    #  In this section, we load the input and output data,\n",
    "    #  prepare data to feed into softmax model.\n",
    "    #  For softmax regression on MNIST pixels,\n",
    "    #  the input data is the images, and\n",
    "    #  the output data is the labels.\n",
    "    #  Size of xtrn: (28,28,1,60000)\n",
    "    #  Size of xtrn must be: (784, 60000)\n",
    "    #  Size of xtst must be: (784, 10000)\n",
    "    #  Size of ytrn must be: (10, 60000)\n",
    "    #  Size of ytst must be: (10, 10000)\n",
    "\n",
    "    xtrn, ytrn, xtst, ytst = mnist() # loading the data\n",
    "    xtrn = reshape(xtrn, 784, 60000)\n",
    "    xtst = reshape(xtst, 784, 10000)\n",
    "\n",
    "    function to_onehot(x)\n",
    "        onehot = zeros(10, 1)\n",
    "        onehot[x, 1] = 1.0\n",
    "        return onehot\n",
    "    end\n",
    "\n",
    "    ytrn = hcat(map(to_onehot, ytrn)...)\n",
    "    ytst = hcat(map(to_onehot, ytst)...)\n",
    "\n",
    "    # STEP 1: Create minibatches\n",
    "    #   Complete the minibatch function\n",
    "    #   It takes the input matrix (X) and gold labels (Y)\n",
    "    #   returns list of tuples contain minibatched input and labels (x, y)\n",
    "    bs = 100\n",
    "    trn_data = minibatch(xtrn, ytrn, bs)\n",
    "\n",
    "    # STEP 2: Initialize parameters\n",
    "    #   Complete init_params function\n",
    "    #   It takes number of inputs and number of outputs(number of classes)\n",
    "    #   It returns randomly generated W matrix and bias vector\n",
    "    #   Sample from N(0, 0.001)\n",
    "\n",
    "    W, b = init_params(ninputs, noutputs)\n",
    "\n",
    "    # STEP 3: Implement softmax_forw and softmax_back_and_loss\n",
    "    #   softmax_forw function takes W, b, and data\n",
    "    #   calculates predicted probabilities\n",
    "    #\n",
    "    #   softmax_back_and_loss function obtains probabilites by calling\n",
    "    #   softmax_forw then calculates soft loss and gradients of W and b\n",
    "\n",
    "    # STEP 4: Gradient checking\n",
    "    #   Skip this part for the lab session.\n",
    "    #   As with any learning algorithm, you should always check that your\n",
    "    #   gradients are correct before learning the parameters.\n",
    "\n",
    "    debug = true # Turn this parameter off, after gradient checking passed\n",
    "    if debug\n",
    "        grad_check(W, b, xtrn[:, 1:100], ytrn[:, 1:100])\n",
    "    end\n",
    "\n",
    "    lr = 0.15\n",
    "\n",
    "    # STEP 5: Training\n",
    "    #   The train function takes model parameters and the data\n",
    "    #   Trains the model over minibatches\n",
    "    #   For each minibatch, first cost and gradients are calculated then model parameters are updated\n",
    "    #   train function returns the average cost per instance\n",
    "\n",
    "    for i=1:50\n",
    "        cost = train(W, b, trn_data, lr)\n",
    "        pred = softmax_forw(W, b, xtrn)\n",
    "        trnacc = accuracy(ytrn, pred)\n",
    "        pred = softmax_forw(W, b, xtst)\n",
    "        tstacc = accuracy(ytst, pred)\n",
    "        @printf(\"epoch: %d softloss: %g trn accuracy: %g tst accuracy: %g\\n\", i, cost, trnacc, tstacc)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading MNIST...\n",
      "└ @ Main C:\\Users\\acer\\.julia\\packages\\Knet\\2xiR8\\data\\mnist.jl:33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 1.4445678878353598\n",
      "Diff must be < 1e-7\n",
      "epoch: 1 softloss: 0.481559 trn accuracy: 0.896983 tst accuracy: 0.9064\n",
      "epoch: 2 softloss: 0.339105 trn accuracy: 0.907617 tst accuracy: 0.9119\n",
      "epoch: 3 softloss: 0.31604 trn accuracy: 0.912017 tst accuracy: 0.9142\n",
      "epoch: 4 softloss: 0.303876 trn accuracy: 0.914783 tst accuracy: 0.9156\n",
      "epoch: 5 softloss: 0.29597 trn accuracy: 0.916567 tst accuracy: 0.9172\n",
      "epoch: 6 softloss: 0.290259 trn accuracy: 0.918033 tst accuracy: 0.9187\n",
      "epoch: 7 softloss: 0.285858 trn accuracy: 0.919233 tst accuracy: 0.9198\n",
      "epoch: 8 softloss: 0.282317 trn accuracy: 0.920083 tst accuracy: 0.92\n",
      "epoch: 9 softloss: 0.279378 trn accuracy: 0.9209 tst accuracy: 0.9204\n",
      "epoch: 10 softloss: 0.276879 trn accuracy: 0.921717 tst accuracy: 0.9211\n",
      "epoch: 11 softloss: 0.274716 trn accuracy: 0.92225 tst accuracy: 0.9207\n",
      "epoch: 12 softloss: 0.272816 trn accuracy: 0.92305 tst accuracy: 0.9214\n",
      "epoch: 13 softloss: 0.271127 trn accuracy: 0.923667 tst accuracy: 0.9214\n",
      "epoch: 14 softloss: 0.269609 trn accuracy: 0.924133 tst accuracy: 0.9215\n",
      "epoch: 15 softloss: 0.268235 trn accuracy: 0.924417 tst accuracy: 0.922\n",
      "epoch: 16 softloss: 0.26698 trn accuracy: 0.9247 tst accuracy: 0.9219\n",
      "epoch: 17 softloss: 0.265828 trn accuracy: 0.924933 tst accuracy: 0.9218\n",
      "epoch: 18 softloss: 0.264764 trn accuracy: 0.92505 tst accuracy: 0.922\n",
      "epoch: 19 softloss: 0.263777 trn accuracy: 0.925367 tst accuracy: 0.9223\n",
      "epoch: 20 softloss: 0.262856 trn accuracy: 0.92575 tst accuracy: 0.9225\n",
      "epoch: 21 softloss: 0.261995 trn accuracy: 0.9263 tst accuracy: 0.9227\n",
      "epoch: 22 softloss: 0.261186 trn accuracy: 0.926567 tst accuracy: 0.9226\n",
      "epoch: 23 softloss: 0.260424 trn accuracy: 0.9269 tst accuracy: 0.9229\n",
      "epoch: 24 softloss: 0.259704 trn accuracy: 0.92715 tst accuracy: 0.9227\n",
      "epoch: 25 softloss: 0.259022 trn accuracy: 0.927367 tst accuracy: 0.9227\n",
      "epoch: 26 softloss: 0.258374 trn accuracy: 0.9275 tst accuracy: 0.9229\n",
      "epoch: 27 softloss: 0.257758 trn accuracy: 0.927767 tst accuracy: 0.923\n",
      "epoch: 28 softloss: 0.257171 trn accuracy: 0.928083 tst accuracy: 0.9229\n",
      "epoch: 29 softloss: 0.25661 trn accuracy: 0.92825 tst accuracy: 0.9231\n",
      "epoch: 30 softloss: 0.256073 trn accuracy: 0.92835 tst accuracy: 0.9229\n",
      "epoch: 31 softloss: 0.255558 trn accuracy: 0.928517 tst accuracy: 0.923\n",
      "epoch: 32 softloss: 0.255064 trn accuracy: 0.928783 tst accuracy: 0.9228\n",
      "epoch: 33 softloss: 0.254589 trn accuracy: 0.92895 tst accuracy: 0.9229\n",
      "epoch: 34 softloss: 0.254133 trn accuracy: 0.9291 tst accuracy: 0.9227\n",
      "epoch: 35 softloss: 0.253692 trn accuracy: 0.929167 tst accuracy: 0.9228\n",
      "epoch: 36 softloss: 0.253268 trn accuracy: 0.92925 tst accuracy: 0.9227\n",
      "epoch: 37 softloss: 0.252858 trn accuracy: 0.929417 tst accuracy: 0.923\n",
      "epoch: 38 softloss: 0.252462 trn accuracy: 0.929567 tst accuracy: 0.9229\n",
      "epoch: 39 softloss: 0.252078 trn accuracy: 0.929667 tst accuracy: 0.9228\n",
      "epoch: 40 softloss: 0.251707 trn accuracy: 0.929783 tst accuracy: 0.9229\n",
      "epoch: 41 softloss: 0.251347 trn accuracy: 0.929867 tst accuracy: 0.9231\n",
      "epoch: 42 softloss: 0.250998 trn accuracy: 0.930067 tst accuracy: 0.9235\n",
      "epoch: 43 softloss: 0.25066 trn accuracy: 0.9301 tst accuracy: 0.9235\n",
      "epoch: 44 softloss: 0.250331 trn accuracy: 0.930233 tst accuracy: 0.9235\n",
      "epoch: 45 softloss: 0.250011 trn accuracy: 0.930333 tst accuracy: 0.9235\n",
      "epoch: 46 softloss: 0.2497 trn accuracy: 0.9305 tst accuracy: 0.9237\n",
      "epoch: 47 softloss: 0.249397 trn accuracy: 0.930583 tst accuracy: 0.9238\n",
      "epoch: 48 softloss: 0.249102 trn accuracy: 0.9307 tst accuracy: 0.9239\n",
      "epoch: 49 softloss: 0.248815 trn accuracy: 0.93085 tst accuracy: 0.9242\n",
      "epoch: 50 softloss: 0.248535 trn accuracy: 0.930933 tst accuracy: 0.9243\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "#= Example Output\n",
    "Diff: 1.8292339049184216e-9\n",
    "Gradient Checking Passed\n",
    "epoch: 1 softloss: 0.481559 trn accuracy: 0.896983 tst accuracy: 0.9064\n",
    "epoch: 2 softloss: 0.339105 trn accuracy: 0.907617 tst accuracy: 0.9119\n",
    "epoch: 3 softloss: 0.31604 trn accuracy: 0.912017 tst accuracy: 0.9142\n",
    "epoch: 4 softloss: 0.303876 trn accuracy: 0.914783 tst accuracy: 0.9156\n",
    "epoch: 5 softloss: 0.29597 trn accuracy: 0.916567 tst accuracy: 0.9172\n",
    "epoch: 6 softloss: 0.290259 trn accuracy: 0.918033 tst accuracy: 0.9187\n",
    "epoch: 7 softloss: 0.285858 trn accuracy: 0.919233 tst accuracy: 0.9198\n",
    "epoch: 8 softloss: 0.282317 trn accuracy: 0.920083 tst accuracy: 0.92\n",
    "epoch: 9 softloss: 0.279378 trn accuracy: 0.9209 tst accuracy: 0.9204\n",
    "epoch: 10 softloss: 0.276879 trn accuracy: 0.921717 tst accuracy: 0.9211\n",
    "epoch: 11 softloss: 0.274716 trn accuracy: 0.92225 tst accuracy: 0.9207\n",
    "epoch: 12 softloss: 0.272816 trn accuracy: 0.92305 tst accuracy: 0.9214\n",
    "epoch: 13 softloss: 0.271127 trn accuracy: 0.923667 tst accuracy: 0.9214\n",
    "epoch: 14 softloss: 0.269609 trn accuracy: 0.924133 tst accuracy: 0.9215\n",
    "epoch: 15 softloss: 0.268235 trn accuracy: 0.924417 tst accuracy: 0.922\n",
    "epoch: 16 softloss: 0.26698 trn accuracy: 0.9247 tst accuracy: 0.9219\n",
    "epoch: 17 softloss: 0.265828 trn accuracy: 0.924933 tst accuracy: 0.9218\n",
    "epoch: 18 softloss: 0.264764 trn accuracy: 0.92505 tst accuracy: 0.922\n",
    "epoch: 19 softloss: 0.263777 trn accuracy: 0.925367 tst accuracy: 0.9223\n",
    "epoch: 20 softloss: 0.262856 trn accuracy: 0.92575 tst accuracy: 0.9225\n",
    "epoch: 21 softloss: 0.261995 trn accuracy: 0.9263 tst accuracy: 0.9227\n",
    "epoch: 22 softloss: 0.261186 trn accuracy: 0.926567 tst accuracy: 0.9226\n",
    "epoch: 23 softloss: 0.260424 trn accuracy: 0.9269 tst accuracy: 0.9229\n",
    "epoch: 24 softloss: 0.259704 trn accuracy: 0.92715 tst accuracy: 0.9227\n",
    "epoch: 25 softloss: 0.259022 trn accuracy: 0.927367 tst accuracy: 0.9227\n",
    "epoch: 26 softloss: 0.258374 trn accuracy: 0.9275 tst accuracy: 0.9229\n",
    "epoch: 27 softloss: 0.257758 trn accuracy: 0.927767 tst accuracy: 0.923\n",
    "epoch: 28 softloss: 0.257171 trn accuracy: 0.928083 tst accuracy: 0.9229\n",
    "epoch: 29 softloss: 0.25661 trn accuracy: 0.92825 tst accuracy: 0.9231\n",
    "epoch: 30 softloss: 0.256073 trn accuracy: 0.92835 tst accuracy: 0.9229\n",
    "epoch: 31 softloss: 0.255558 trn accuracy: 0.928517 tst accuracy: 0.923\n",
    "epoch: 32 softloss: 0.255064 trn accuracy: 0.928783 tst accuracy: 0.9228\n",
    "epoch: 33 softloss: 0.254589 trn accuracy: 0.92895 tst accuracy: 0.9229\n",
    "epoch: 34 softloss: 0.254133 trn accuracy: 0.9291 tst accuracy: 0.9227\n",
    "epoch: 35 softloss: 0.253692 trn accuracy: 0.929167 tst accuracy: 0.9228\n",
    "epoch: 36 softloss: 0.253268 trn accuracy: 0.92925 tst accuracy: 0.9227\n",
    "epoch: 37 softloss: 0.252858 trn accuracy: 0.929417 tst accuracy: 0.923\n",
    "epoch: 38 softloss: 0.252462 trn accuracy: 0.929567 tst accuracy: 0.9229\n",
    "epoch: 39 softloss: 0.252078 trn accuracy: 0.929667 tst accuracy: 0.9228\n",
    "epoch: 40 softloss: 0.251707 trn accuracy: 0.929783 tst accuracy: 0.9229\n",
    "epoch: 41 softloss: 0.251347 trn accuracy: 0.929867 tst accuracy: 0.9231\n",
    "epoch: 42 softloss: 0.250998 trn accuracy: 0.930067 tst accuracy: 0.9235\n",
    "epoch: 43 softloss: 0.25066 trn accuracy: 0.9301 tst accuracy: 0.9235\n",
    "epoch: 44 softloss: 0.250331 trn accuracy: 0.930233 tst accuracy: 0.9235\n",
    "epoch: 45 softloss: 0.250011 trn accuracy: 0.930333 tst accuracy: 0.9235\n",
    "epoch: 46 softloss: 0.2497 trn accuracy: 0.9305 tst accuracy: 0.9237\n",
    "epoch: 47 softloss: 0.249397 trn accuracy: 0.930583 tst accuracy: 0.9238\n",
    "epoch: 48 softloss: 0.249102 trn accuracy: 0.9307 tst accuracy: 0.9239\n",
    "epoch: 49 softloss: 0.248815 trn accuracy: 0.93085 tst accuracy: 0.9242\n",
    "epoch: 50 softloss: 0.248535 trn accuracy: 0.930933 tst accuracy: 0.9243\n",
    "=#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
